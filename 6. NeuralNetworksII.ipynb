{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t08IWzpL-6jF"
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural nets are a specific method for learning from data, a method that is based on a very simple element, the *neuron unit*. A neuron unit (or 1-layer neural network) is a mathematical function of this kind:\n",
    "\n",
    "${\\mathbf y} = \\sigma(\\mathbf{w}^T \\cdot {\\mathbf x} + b)$\n",
    "\n",
    "where ${\\mathbf x}$ represents an input element in vector form, $\\mathbf{w}$ is a vector of weights,  $\\sigma$ is a non-linear function and $b$ a scalar value. $(\\mathbf{w},b)$ are called the parameters of the function. The output of this function is called the *activation* of the neuron. \n",
    "\n",
    "Regarding the non-linear function, historically the most common one was the Sigmoid function, but nowadays there are several alternatives that are supposed to be better suited to learning from data, such as ReLU and variants.\n",
    "\n",
    "> **Q:** What kind of decision functions are represented by a 1-layer nn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "FX5NwyDo_A_P",
    "outputId": "acd96624-86c2-4f1c-bf71-1fcd66e9b67b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "plt.ylim(-1.5, 10)\n",
    "x = np.linspace(-10.0,10.0,100)\n",
    "y1 = sigmoid(x)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x,y1)\n",
    "y2 = ReLU(x)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x,y2,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RX5LERmbBQaR",
    "outputId": "1f8a1fc9-0322-48be-b0a0-d6398ab4bfba"
   },
   "outputs": [],
   "source": [
    "x = np.array([0.4,1.2,3.5])\n",
    "\n",
    "w = np.array([1.0,2.0,1.0])\n",
    "b = 1.3\n",
    "\n",
    "y = sigmoid(np.dot(x,w) + b)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2P7yHSro_Inc"
   },
   "source": [
    "## Multilayer neural networks\n",
    "\n",
    "Simple neurons can be organized in larger structures by applying to the same data vector different sets of weights, forming what is called a *layer*, and by stacking layers one on top of the output of the other.  \n",
    "\n",
    "It is important to notice that a multilayer neural network can be seen as a composition of matrix products (matrices represent weights) and non-linear function activations. For the case of a 2-layer network the outcome is:\n",
    "\n",
    "$ {\\mathbf y} = {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big)$\n",
    "\n",
    "where ${\\mathbf \\sigma}$ represents a vectorial version of the sigmoid function and $W^i$ are the weights of each layer in matrix form.  \n",
    "\n",
    "What is interesting about this kind of structures is that it has been showed that even a neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function of $\\mathbf{R}^n$. This fact makes neural networks a sound candidate to implement learning from data methods. The question is then: how to find the optimal parameters, ${\\mathbf w} = (W^i,{\\mathbf b})$, to approximate a function that is implicitly defined by a set of samples $\\{({\\mathbf x}_1, {\\mathbf y}_1), \\dots,  ({\\mathbf x}_n, {\\mathbf y}_n)\\}$?\n",
    "\n",
    "From a technical point of view, not only neural networks but most of the algorithms that have been proposed to infer models from large data sets are based on the iterative solution of a mathematical problem that involves data and a mathematical model. If there was an analytic solution to the problem, this should be the adopted one, but this is not the case for most of the cases. The techniques that have been designed to tackle these problems are grouped under a field that is called optimization. The most important technique for solving optimization problems is *gradient descend*.\n",
    "\n",
    "> The training of models like $ {\\mathbf y} = {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big)$ (or bigger!) can be readily performed by applying *automatic differentiation* to a loss function. \n",
    "\n",
    "> In the case of regression: $L = \\frac{1}{n} \\sum_{i=1}^n \\Big({\\mathbf y}_i - {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x}_i + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 )\\Big)\\Big)^2 $\n",
    "\n",
    "> In the case of two-class classification: $L = \\frac{1}{n} log(1 + exp(-y_i {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big))) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMoVJNwZB81F"
   },
   "source": [
    "## Playing with neural nets.\n",
    "+ Concentric classes, 1 layer, Sigmoid.\n",
    "+ Concentric classes, 1 layer, ReLu.\n",
    "+ X-or, 0 layer.\n",
    "+ X-or, 1 layer.\n",
    "+ Spiral data.\n",
    "+ Regression.\n",
    "\n",
    "\n",
    "http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqXdykt2DYo0"
   },
   "source": [
    "# Deep Learning in `keras`\n",
    "\n",
    "> Keras is a high-level neural networks library, written in Python and capable of running on top TensorFlow. It was developed with a focus on enabling fast experimentation.\n",
    "\n",
    "The core data structure of Keras is a model, a way to organize layers. The main type of model is the ``Sequential model``, a linear stack of layers. \n",
    "\n",
    "```Python\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "Stacking layers is as easy as ``.add()``:\n",
    "\n",
    "```Python\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(output_dim=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "```\n",
    "\n",
    "Once your model looks good, configure its learning process with ``.compile()``:\n",
    "\n",
    "```Python\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "If you need to, you can further configure your optimizer.\n",
    "\n",
    "```Python\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "```\n",
    "\n",
    "You can now iterate on your training data in batches:\n",
    "\n",
    "```Python\n",
    "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n",
    "```\n",
    "\n",
    "Evaluate your performance in one line:\n",
    "```Python\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "```\n",
    "\n",
    "Or generate predictions on new data:\n",
    "\n",
    "```Python\n",
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "id": "uqHskK3EZQx_",
    "outputId": "1b785a46-512e-480e-a300-982d94006a19"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "3htJMJEualOT",
    "outputId": "34b502a9-9bc1-414f-a176-e1f0e5c222a5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xEufgoGHDytG",
    "outputId": "670359b3-8fbc-4eb1-ae9f-b14552eb0af3"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'],label='train_acc')\n",
    "plt.plot(history.history['val_acc'],label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ljANZSaIYe8I"
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is a way to regularize the neural network. During training, it may happen that neurons of a particular layer may always become influenced only by the output of a particular neuron in the previous layer. In that case, the neural network would overfit.\n",
    "\n",
    "Dropout prevents overfitting and regularizes by randomly cutting the connections (also known as dropping the connection) between neurons in successuve layers during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i22znQv4bA1Q"
   },
   "source": [
    "### Keras optimizers\n",
    "\n",
    "There are several variants of gradient descent, which differ in how we compute the step.\n",
    "\n",
    "Keras supports seven optimizers.\n",
    "\n",
    "```python\n",
    "my_opt = K.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "my_opt = K.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "my_opt = K.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "my_opt = K.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "my_opt = K.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "my_opt = K.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "my_opt = K.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "```\n",
    "\n",
    "#### Momentum\n",
    "\n",
    "For example, SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.\n",
    "\n",
    "**Momentum** is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction of the update vector of the past time step to the current update vector:\n",
    "\n",
    "$$ v_t = m v_{t-1} + \\alpha \\nabla_w f $$$$ w = w - v_t    $$\n",
    "\n",
    "The momentum $m$ is commonly set to $0.9$.\n",
    "\n",
    "#### Adagrad\n",
    "\n",
    "SGD manipulates the learning rate globally and equally for all parameters. Tuning the learning rates is an expensive process, so much work has gone into devising methods that can adaptively tune the learning rates, and even do so per parameter.\n",
    "\n",
    "Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.\n",
    "\n",
    "$$ c = c + (\\nabla_w f)^2 $$$$ w = w - \\frac{\\alpha}{\\sqrt{c}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3cSy4_H6aEi"
   },
   "source": [
    "## CNN\n",
    "\n",
    "The previously mentioned multilayer perceptrons represent the most general and powerful feedforward neural network model possible; they are organised in layers, such that every neuron within a layer receives its own copy of all the outputs of the previous layer as its input. This kind of model is perfect for the right kind of problem – learning from a fixed number of (more or less) unstructured parameters.\n",
    "\n",
    "> However, consider what happens to the number of parameters (weights) of such a model when being fed raw image data (f.e. a $200 \\times 200$ pixel image connected to 1024 neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gxR3h2-d6nGn",
    "outputId": "44d7ccc2-a157-4172-acfb-3ce54e826a27"
   },
   "outputs": [],
   "source": [
    "200 * 200 * 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlCN_f8E7OZ2"
   },
   "source": [
    "The situation quickly becomes unmanageable as image sizes grow larger, way before reaching the kind of images people usually want to work with in real applications.\n",
    "\n",
    "A common solution is to downsample the images to a size where MLPs can safely be applied. However, if we directly downsample the image, we potentially lose a wealth of information; it would be great if we would somehow be able to still do some useful (without causing an explosion in parameter count) processing of the image, prior to performing the downsampling.\n",
    "\n",
    "It turns out that there is a very efficient way of pulling this off, and it makes advantage of the structure of the information encoded within an image – it is assumed that pixels that are spatially closer together will \"cooperate\" on forming a particular feature of interest much more than ones on opposite corners of the image. Also, if a particular (smaller) feature is found to be of great importance when defining an image's label, it will be equally important if this feature was found anywhere within the image, regardless of location.\n",
    "\n",
    "Enter the convolution operator. Given a two-dimensional image, $I$, and a small matrix, $K$ of size $h \\times w$, (known as a convolution kernel), which we assume encodes a way of extracting an interesting image feature, we compute the convolved image, $I∗K$, by overlaying the kernel on top of the image in all possible ways, and recording the sum of elementwise products between the image and the kernel:\n",
    "\n",
    "$$\n",
    "output(x,y) = (I \\otimes K)(x,y) = \\sum_{m=0}^{M-1} \\sum_{n=1}^{N-1} K(m,n) I(x-n, y-m)\n",
    "$$\n",
    "\n",
    "The convolution operator forms the fundamental basis of the convolutional layer of a CNN. The layer is completely specified by a certain number of kernels, $K$, and it operates by computing the convolution of the output images of a previous layer with each of those kernels, afterwards adding the biases (one per each output image). Finally, an activation function, $\\sigma$, may be applied to all of the pixels of the output images. \n",
    "\n",
    "Typically, the input to a convolutional layer will have $d$ channels (e.g., red/green/blue in the input layer), in which case the kernels are extended to have this number of channels as well.\n",
    "\n",
    "Note that, since all we're doing here is addition and scaling of the input pixels, the kernels may be learned from a given training dataset via gradient descent, exactly as the weights of an MLP. In fact, an MLP is perfectly capable of replicating a convolutional layer, but it would require a lot more training time (and data) to learn to approximate that mode of operation.\n",
    "\n",
    "## Pooling\n",
    "\n",
    "In fact, after a convolutional layer there are two kinds of non linear functions that are usually applied: non-linear activation functions such as sigmoids or ReLU and *pooling*. Pooling layers are used with the purpose to progressively reduce the spatial size of the image to achieve scale invariance. The most common layer is the *maxpool* layer. Basically a maxpool of $2 \\times 2$ causes a filter of 2 by 2 to traverse over the entire input array and pick the largest element from the window to be included in the next representation map. Pooling can also be implemented by using other criteria, such as averaging instead of taking the max element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "KvbTpVNS7COv",
    "outputId": "b58744af-f48d-4f59-a4b0-764e1929a8f5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10 # subroutines for fetching the CIFAR-10 dataset\n",
    "from tensorflow.keras.models import Model # basic class for specifying and training a neural network\n",
    "from tensorflow.keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 200 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() # fetch CIFAR-10 data\n",
    "\n",
    "num_train, height, width, depth = X_train.shape # there are 50000 training examples in CIFAR-10 \n",
    "num_test = X_test.shape[0] # there are 10000 test examples in CIFAR-10\n",
    "num_classes = np.unique(y_train).shape[0] # there are 10 image classes\n",
    "\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= np.max(X_train) # Normalise data to [0, 1] range\n",
    "X_test /= np.max(X_test) # Normalise data to [0, 1] range\n",
    "\n",
    "Y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "Y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
    "\n",
    "inp = Input(shape=(height, width, depth)) # depth goes last in TensorFlow back-end (first in Theano)\n",
    "# Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\n",
    "conv_1 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(inp)\n",
    "conv_2 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "# Conv [64] -> Conv [64] -> Pool (with dropout on the pooling layer)\n",
    "conv_3 = Convolution2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(drop_1)\n",
    "conv_4 = Convolution2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(conv_3)\n",
    "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
    "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
    "# Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\n",
    "flat = Flatten()(drop_2)\n",
    "hidden = Dense(hidden_size, activation='relu')(flat)\n",
    "drop_3 = Dropout(drop_prob_2)(hidden)\n",
    "out = Dense(num_classes, activation='softmax')(drop_3)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "model.fit(X_train, Y_train,                # Train the model using the training set...\n",
    "          batch_size=batch_size, epochs=num_epochs,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "model.evaluate(X_test, Y_test, verbose=1)  # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice - \n",
    "# Implement an Autoencoder for MNIST DATA SET\n",
    "\n",
    "# You can use the following loss function\n",
    "# loss='mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8-AH0Zy96Aa"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "\n",
    "Classical neural networks, including convolutional ones, suffer from two severe limitations:\n",
    "\n",
    "+ They only accept a fixed-sized vector as input and produce a fixed-sized vector as output.\n",
    "+ They do not consider the sequential nature of some data (language, video frames, time series, etc.) \n",
    "\n",
    "Recurrent neural networks (RNN) overcome these limitations by allowing to operate over sequences of vectors (in the input, in the output, or both). RNNs are called recurrent because they perform the same task for every element of the sequence, with the output depending on the previous computations. The basic formulas of a simple RNN are:\n",
    "\n",
    "$$ s_t = f_1 (Ux_t + W s_{t-1}) $$\n",
    "$$ y_t = f_2 (V s_t) $$\n",
    "\n",
    "These equations basically say that the current network state, commonly known as hidden state, $s_t$ is a function $f_1$ of the previous hidden state $s_{t-1}$ and the current input $x_t$. $U, V, W$ matrices are the parameters of the function. \n",
    "\n",
    "Given an input sequence, we apply RNN formulas in a recurrent way until we process all input elements. The RNN shares the parameters  $U,V,W$ across all recurrent steps. We can think of the hidden state  as a memory of the network that captures information about the previous steps.\n",
    "\n",
    "The novelty of this type of network is that we we have encoded in the very architecture of the network a sequence modeling scheme that has been in used in the past to predict time series as well as to model language. In contrast to the precedent architectures we have introduced, now the hidden layers are indexed by both 'spatial' and 'temporal' index. \n",
    "\n",
    "The inputs of a recurrent network are always vectors, but we can process sequences of symbols/words by representing these symbols by numerical vectors.\n",
    "\n",
    "Let's suppose we want to classify a phrase or a series of words. Let $x^1, ...,x^{C}$ the word vectors corresponding to a corpus with $C$ symbols. Then, the relationship to compute the hidden layer output features at each time-step $t$ is $h_t = \\sigma(W s_{t-1} + U x_{t})$, where:\n",
    "\n",
    "+ $x_{t} \\in \\mathbf{R}^{d}$ is input word vector at time $t$. \n",
    "+ $U \\in \\mathbf{R}^{D_h \\times d}$ is the weights matrix of the input word vector, $x_t$.\n",
    "+ $W \\in \\mathbf{R}^{D_h \\times D_h}$ is the weights matrix of the output of the previous time-step, $t-1$.\n",
    "+ $s_{t-1}  \\in \\mathbf{R}^{D_h}$ is the output of the non-linear function at the previous time-step, $t-1$. \n",
    "+ $\\sigma ()$ is the non-linearity function (normally, ``tanh``).\n",
    "\n",
    "\n",
    "The output of this network is $\\hat{y}_t = softmax (V h_t)$, that represents the output probability distribution over the vocabulary at each time-step $t$.  \n",
    "\n",
    "Essentially, $\\hat{y}_t$ is the next predicted word given the document context score so far (i.e. $h_{t-1}$) and the last observed word vector $x^{(t)}$. \n",
    "\n",
    "The loss function used in RNNs is often the cross entropy error:\n",
    "\n",
    "$\n",
    "\tL^{(t)}(W) = - \\sum_{j=1}^{|V|} y_{t,j} \\times log (\\hat{y}_{t,j})\n",
    "$\n",
    "\n",
    "The cross entropy error over a corpus of size $C$ is:\n",
    "\n",
    "$\n",
    "\tL = \\frac{1}{C} \\sum_{c=1}^{C} L^{(c)}(W) = - \\frac{1}{C} \\sum_{c=1}^{C} \\sum_{j=1}^{|V|} y_{c,j} \\times log (\\hat{y}_{c,j})\n",
    "$\n",
    "\n",
    "These simple RNN architectures have been shown to be too prone to forget information when sequences are long and they are also very unstable when trained. For this reason several alternative architectures have been proposed. These alternatives are based on the presence of *gated units*. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. The two most important alternative RNN are Long Short Term Memories (LSTM) and Gated Recurrent Units (GRU) networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ayj2JDJ87zKu",
    "outputId": "24461e27-0b85-42db-cfb5-1fdc4b890aa0"
   },
   "outputs": [],
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
